[{"dur": "2.052", "text": "This is a thought experiment.", "start": "7.257"}, {"dur": "2.608", "text": "Let's say at some point\nin the not so distant future,", "start": "9.309"}, {"dur": "3.588", "text": "you're barreling down the highway\nin your self-driving car,", "start": "11.917"}, {"dur": "4.304", "text": "and you find yourself boxed in\non all sides by other cars.", "start": "15.505"}, {"dur": "4.404", "text": "Suddenly, a large, heavy object\nfalls off the truck in front of you.", "start": "19.809"}, {"dur": "3.151", "text": "Your car can't stop in time\nto avoid the collision,", "start": "24.213"}, {"dur": "2.051", "text": "so it needs to make a decision:", "start": "27.364"}, {"dur": "2.258", "text": "go straight and hit the object,", "start": "29.415"}, {"dur": "2.28", "text": "swerve left into an SUV,", "start": "31.673"}, {"dur": "2.987", "text": "or swerve right into a motorcycle.", "start": "33.953"}, {"dur": "3.51", "text": "Should it prioritize your safety\nby hitting the motorcycle,", "start": "36.94"}, {"dur": "2.817", "text": "minimize danger to others by not swerving,", "start": "40.45"}, {"dur": "4.079", "text": "even if it means hitting the large object\nand sacrificing your life,", "start": "43.267"}, {"dur": "2.758", "text": "or take the middle ground\nby hitting the SUV,", "start": "47.346"}, {"dur": "2.983", "text": "which has a high passenger safety rating?", "start": "50.104"}, {"dur": "3.211", "text": "So what should the self-driving car do?", "start": "53.087"}, {"dur": "3.207", "text": "If we were driving that boxed in car\nin manual mode,", "start": "56.298"}, {"dur": "3.529", "text": "whichever way we'd react\nwould be understood as just that,", "start": "59.505"}, {"dur": "1.286", "text": "a reaction,", "start": "63.034"}, {"dur": "2.251", "text": "not a deliberate decision.", "start": "64.32"}, {"dur": "4.298", "text": "It would be an instinctual panicked move\nwith no forethought or malice.", "start": "66.571"}, {"dur": "3.65", "text": "But if a programmer were to instruct\nthe car to make the same move,", "start": "70.869"}, {"dur": "2.797", "text": "given conditions it may \nsense in the future,", "start": "74.519"}, {"dur": "4.303", "text": "well, that looks more\nlike premeditated homicide.", "start": "77.316"}, {"dur": "1.018", "text": "Now, to be fair,", "start": "81.619"}, {"dur": "4.072", "text": "self-driving cars are are predicted \nto dramatically reduce traffic accidents", "start": "82.637"}, {"dur": "1.247", "text": "and fatalities", "start": "86.709"}, {"dur": "3.442", "text": "by removing human error \nfrom the driving equation.", "start": "87.956"}, {"dur": "2.114", "text": "Plus, there may be all sorts \nof other benefits:", "start": "91.398"}, {"dur": "1.638", "text": "eased road congestion,", "start": "93.512"}, {"dur": "1.573", "text": "decreased harmful emissions,", "start": "95.15"}, {"dur": "4.622", "text": "and minimized unproductive\nand stressful driving time.", "start": "96.723"}, {"dur": "2.371", "text": "But accidents can and will still happen,", "start": "101.345"}, {"dur": "0.969", "text": "and when they do,", "start": "103.716"}, {"dur": "4.462", "text": "their outcomes may be determined\nmonths or years in advance", "start": "104.685"}, {"dur": "2.605", "text": "by programmers or policy makers.", "start": "109.147"}, {"dur": "2.5", "text": "And they'll have \nsome difficult decisions to make.", "start": "111.752"}, {"dur": "2.952", "text": "It's tempting to offer up general \ndecision-making principles,", "start": "114.252"}, {"dur": "1.895", "text": "like minimize harm,", "start": "117.204"}, {"dur": "3.376", "text": "but even that quickly leads \nto morally murky decisions.", "start": "119.099"}, {"dur": "1.158", "text": "For example,", "start": "122.475"}, {"dur": "2.008", "text": "let's say we have the same initial set up,", "start": "123.633"}, {"dur": "2.871", "text": "but now there's a motorcyclist \nwearing a helmet to your left", "start": "125.641"}, {"dur": "2.797", "text": "and another one without \na helmet to your right.", "start": "128.512"}, {"dur": "3.051", "text": "Which one should \nyour robot car crash into?", "start": "131.309"}, {"dur": "4.082", "text": "If you say the biker with the helmet\nbecause she's more likely to survive,", "start": "134.36"}, {"dur": "3.329", "text": "then aren't you penalizing \nthe responsible motorist?", "start": "138.442"}, {"dur": "2.344", "text": "If, instead, you save the biker \nwithout the helmet", "start": "141.771"}, {"dur": "1.991", "text": "because he's acting irresponsibly,", "start": "144.115"}, {"dur": "4.911", "text": "then you've gone way beyond the initial\ndesign principle about minimizing harm,", "start": "146.106"}, {"dur": "3.854", "text": "and the robot car is now \nmeting out street justice.", "start": "151.017"}, {"dur": "3.532", "text": "The ethical considerations \nget more complicated here.", "start": "154.871"}, {"dur": "1.404", "text": "In both of our scenarios,", "start": "158.403"}, {"dur": "4.593", "text": "the underlying design is functioning\nas a targeting algorithm of sorts.", "start": "159.807"}, {"dur": "0.899", "text": "In other words,", "start": "164.4"}, {"dur": "2.51", "text": "it's systematically favoring \nor discriminating", "start": "165.299"}, {"dur": "3.489", "text": "against a certain type \nof object to crash into.", "start": "167.809"}, {"dur": "2.315", "text": "And the owners of the target vehicles", "start": "171.298"}, {"dur": "3.049", "text": "will suffer the negative consequences\nof this algorithm", "start": "173.613"}, {"dur": "2.085", "text": "through no fault of their own.", "start": "176.662"}, {"dur": "4.654", "text": "Our new technologies are opening up\nmany other novel ethical dilemmas.", "start": "178.747"}, {"dur": "2.081", "text": "For instance, if you had to \nchoose between", "start": "183.401"}, {"dur": "4.052", "text": "a car that would always save\nas many lives as possible in an accident,", "start": "185.482"}, {"dur": "3.031", "text": "or one that would save you at any cost,", "start": "189.534"}, {"dur": "1.697", "text": "which would you buy?", "start": "192.565"}, {"dur": "3.315", "text": "What happens if the cars start analyzing\nand factoring in", "start": "194.262"}, {"dur": "3.47", "text": "the passengers of the cars\nand the particulars of their lives?", "start": "197.577"}, {"dur": "2.172", "text": "Could it be the case \nthat a random decision", "start": "201.047"}, {"dur": "4.906", "text": "is still better than a predetermined one\ndesigned to minimize harm?", "start": "203.219"}, {"dur": "2.753", "text": "And who should be making \nall of these decisions anyhow?", "start": "208.125"}, {"dur": "1.094", "text": "Programmers?", "start": "210.878"}, {"dur": "0.936", "text": "Companies?", "start": "211.972"}, {"dur": "1.444", "text": "Governments?", "start": "212.908"}, {"dur": "3.214", "text": "Reality may not play out exactly\nlike our thought experiments,", "start": "214.352"}, {"dur": "1.666", "text": "but that's not the point.", "start": "217.566"}, {"dur": "4.371", "text": "They're designed to isolate \nand stress test our intuitions on ethics,", "start": "219.232"}, {"dur": "2.986", "text": "just like science experiments do\nfor the physical world.", "start": "223.603"}, {"dur": "3.39", "text": "Spotting these moral hairpin turns now", "start": "226.589"}, {"dur": "3.571", "text": "will help us maneuver the unfamiliar road\nof technology ethics,", "start": "229.979"}, {"dur": "3.751", "text": "and allow us to cruise confidently\nand conscientiously", "start": "233.55"}, {"dur": "2.347", "text": "into our brave new future.", "start": "237.301"}]